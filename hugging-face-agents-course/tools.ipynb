{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/renweizhukov/jupyter-lab-notebook/blob/main/hugging-face-agents-course/tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpYomItAeUTe"
   },
   "source": [
    "# Tools in LlamaIndex\n",
    "\n",
    "\n",
    "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
    "\n",
    "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
    "\n",
    "## Let's install the dependencies\n",
    "\n",
    "We will install the dependencies for this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90vBbaSOeUTg",
    "outputId": "2e6e6113-86e0-4ab1-a729-52427e4aa730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface llama-index-tools-google -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxosBATqeUTh"
   },
   "source": [
    "And, let's log in to Hugging Face to use serverless Inference APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "8502cdcd962f4bf988d625c674234dc2",
      "d84f4da91b594cb09c93cbee2e4cbea9",
      "c1646d2c8d5246c2bd57d59e86e48261",
      "0a8f4d0c295345ff960e554bddfbc62e",
      "bf444244366d401f9a4377515b78cffb",
      "07f383fc446549f583f8ae93955f8686",
      "a9842179ff464442bd008bcd3a324f4a",
      "a4c915da068441aab901a6fb62ba0588",
      "1eb723f0d84a486691ba356e076e3afe",
      "fb963fd366f04e55bda8c3d07ed0a1a2",
      "51ce2dea77624382b88289c8ef02c392",
      "9314028ca9b64112a4c24c6acc22f8e0",
      "d245cea80426436e91a0535d87e1c6e8",
      "4a811ea3b4634f1a9163fed29613514b",
      "ee82408e91d343a783cc26c18f1f7f3d",
      "b4fb207bda73497897a94b0d589105f6",
      "daf4520b50e74b9a8ed7b2c775cb29b7",
      "4f1ef345ca3946d0a2fb190565a2b967",
      "101ba9109a6e447b8d28f2016c6611c7",
      "ade03dfb1b5a44aaafb03fbf2d7cfe84"
     ]
    },
    "id": "ZkTMQIzueUTh",
    "outputId": "4a50baa4-8a8b-477b-f404-65c97c5c62ab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8502cdcd962f4bf988d625c674234dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IewValRAeUTh"
   },
   "source": [
    "## Creating a FunctionTool\n",
    "\n",
    "Let's create a basic `FunctionTool` and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWuiCmZTeUTh",
    "outputId": "971186f3-a312-4804-e880-b8d5a2b243a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5WCyNRkeUTi"
   },
   "source": [
    "## Creating a QueryEngineTool\n",
    "\n",
    "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "7eaf28c8a1184507af498ce882b3d34d",
      "a6ef3cdc9e1a4674ada6d271e301949e",
      "2db35178f7db4b3b8bd33033f431979c",
      "8c91e6f18eed451389420154b5c28756",
      "67d2cd457d7e420293dd5dc944388411",
      "8c97045243b6465da7fc058ade820b23",
      "874d22605ca94d09ae732ce309027d96",
      "ca5347e8fe524db2ae60db0f7005eff8",
      "d6c865a59deb4d5c89ed5d03931667b5",
      "5ab7c64b5e034531b79a35810a1e43f4",
      "1403555c2b644e2a821137895aecfb4a",
      "b0bd9b4728b9427c91e290e95a166465",
      "51cd65cd5cc246078c7e4d9dc29d016f",
      "9ced9ae272004f0097b3ec55d5d08b00",
      "fa554a9726594f1e823e2704118bbc93",
      "2c4e70460708471b8ba5682119d3f9d0",
      "8126e8a2b1ca4c98abc5e91ef939ae38",
      "01f2266bb3ee46688a0afd302ed85d63",
      "4cf1760d36d1426b9d7f02f6cccc948c",
      "012171e2f26e4b47ab6b067115ae5392",
      "00a1d9af94fa489aa2939e8063c703cd",
      "f556190ac6594e0791cee281303e8fab",
      "eb8a08714a3d4b4daa120d40e6f1eb5a",
      "22533e4793f649709ac1c89f3e7d78c7",
      "3aea8227f455441b86166ff3f4f96760",
      "54494dce797a49d995bbf4b772d53fcd",
      "436e523455e24f4bba4c054fe619403e",
      "98496128f0db44cb9bc8f23039657d9c",
      "da8a780bf861464587b50f9c65d9a9af",
      "34e3d94a889e4516881d64eb362860b7",
      "da85db6904394358aae980192c9ea546",
      "ed6455b718d84cf691915ee83b4b0767",
      "4c65dd1e4de64e9d9fcb9180b58568f2",
      "3cad2c22c992408c99e9f43496c45b73",
      "07a19dd7080045cba8b0946d67ac1f66",
      "65d14682750145c3bbc59e40d224fd0e",
      "886b02daa5404496accf35b882c59e33",
      "d2492c2996b64221a6847fd8eaa6b961",
      "ad78a8f3eb404c66b88c1ee8dd4e0b01",
      "40fa4298514c4350b87ea4319248fec0",
      "86ca53f4807c4daaa75a2a461c228263",
      "a141cfad16ef4751a1a982b239ef99be",
      "232e5b3f9b47442ab3dab16ea815ef88",
      "4dd17ad07f18474dbc12332f0a573b10",
      "7af4427b6dd844b989431c8bf4ce4db0",
      "dfa0fc45bcc048af9c2cb9bfdee6f708",
      "fd37be5ac6074a9fb37c19b63b5ec492",
      "78bd024dc8d24ca29a730d5aca5011f8",
      "13a480180dd646c6bce1174a95bbcf57",
      "63f1311e06ac4c95ae3db1d13de3e9d6",
      "33fdec434bb540cb9b3204df8c719b8a",
      "3846c58e6af144a5bde13254f071a335",
      "1d7dab4159e54d7aaef328004b01ddea",
      "a6efa1820586476aad6c26c976843ddd",
      "8a6ac03828ab4496a5e0f340b321192f",
      "82eb7bea2bd049028fca358c027a8fb7",
      "309121441c724eae941a73f34c5abb82",
      "fa9dec7cd3d74f9cba18fe537e4352ca",
      "9c6e64929fa543cebd7768a4467a19df",
      "8045a8985e79497aa0e9152076a9a810",
      "5462a9562bc74cf3ae90799ff31f4b8b",
      "5b6df12b5faa4193bf2b78e7a19efed5",
      "adfc26ae5dcd496ca50a8ca82e9d5209",
      "9ca4a451d50d46b791904693abb86b7a",
      "2f264ca2ee35479d9037730548814566",
      "667611174d8047edb260f94ae75e34df",
      "e98fae17ec554ced90148379e406a7ad",
      "5121c808a8454dcea55cb33feda32b7f",
      "dad066c05a4841b591de883722ab8fab",
      "c45e1b4b813a483db93ae38467769e4e",
      "7e66b683788f4b119aa0003fd27eaf40",
      "1afecc1707b1469e8830ff8a0a918ef0",
      "5647636192944e9db8dda5f150555fa8",
      "8eb4695cd7e24341949ce5b9d6ffad3b",
      "45712c0f359b4179b335f8c1537e8c67",
      "d79c0ad7185c4cceb7a4678f66f93aab",
      "7612a738aff74793afc73d0b18d9360c",
      "bae9a3e20708430ebc9006af8644fa4a",
      "fb9a5ec6b0304d9080e428f1f80b3d9b",
      "8c3a33094cbb44cfae97fc13ebf675d6",
      "8762dc1343fb417d8e5ce1709af78046",
      "668f81ac4a5a45bf9292ebf194313551",
      "f739db155ef241dda82f31a6e0b69f99",
      "10526b5cefe944d8a8815ecb8a4ed7d0",
      "6d35ae3df84b40feaf2b3c725207c58a",
      "ea88599053444edb9fe7f18399668ad0",
      "fcd0a8aee0d74668b3fe61cdc7eaedeb",
      "ea4a726880024295b03111149e000d69",
      "1c3e554fc5574e47aed58cbbf1076050",
      "a3b3cb098cd64244bd29daa089072cc5",
      "dc9c48a22c8c409ba5ab0204a53cbbe8",
      "5593407e66df436895f41a620d69fb68",
      "7096191e118b4dd2b0b4ddb39cb3cf22",
      "6cd98e7d2edf425eb11f5e6fef5127ee",
      "ae1e221af5944dd6975dea24030d1ac5",
      "f0b836700d224e9abba9b972544ba85c",
      "09ee878a1e334185b1758522deff7ab0",
      "e088291037784f4386d0546ccc530e6f",
      "75aa89db30314001882c72f8257278bb",
      "5e82c29d2b964092ae51060a481e04d3",
      "cf702467331742b8822bdaabec3ef95e",
      "85d42024c50645118cf01217339064fb",
      "5aaff56ffd1e446199bc0c7a720866a6",
      "176840f888bb4c27b295fe764947fa05",
      "65494852dfa14dcb93774738a2fe3df6",
      "8a52539cc09b4e0f903ee639e416a770",
      "95e8842f585248a0867b7a18b4a23613",
      "4227b65e3e8f44468f1202dc583d8bfe",
      "837be6c062fb4c228f6b3dab3557e818",
      "515d1669c48c4d68a240781ffb649a93",
      "f68ab3ef719a41f78ea0b283907d98c9",
      "4976e399e4f646139a25e9b5fcafec8e",
      "3788c164132f43a096aa8aff023eba11",
      "b6361ba8a5cb44508eb1a6418b057b73",
      "e3528ba81d9a4bb7b64f00b7cb922cb7",
      "7e23e88c562640ddbeea887bb6a33eb0",
      "95bbea0c88354e9c802a8651ed5bdbff",
      "98af3e230d204d0286525dc074c4bbaf",
      "edf157a1077440748a4c2e08eace5dc2",
      "57853b330e324651ae331b0296ac080f",
      "fb23220d3a5a4151b45bc9b890cc4803"
     ]
    },
    "id": "T1LsP7qTeUTi",
    "outputId": "b7f90831-3509-4a91-9ff8-09fc7b314e0b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaf28c8a1184507af498ce882b3d34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bd9b4728b9427c91e290e95a166465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a08714a3d4b4daa120d40e6f1eb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cad2c22c992408c99e9f43496c45b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af4427b6dd844b989431c8bf4ce4db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eb7bea2bd049028fca358c027a8fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98fae17ec554ced90148379e406a7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae9a3e20708430ebc9006af8644fa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3e554fc5574e47aed58cbbf1076050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e82c29d2b964092ae51060a481e04d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68ab3ef719a41f78ea0b283907d98c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='Empty Response', tool_name='some useful name', raw_input={'input': 'Responds about research on the impact of AI on the future of work and society?'}, raw_output=Response(response='Empty Response', source_nodes=[], metadata=None), is_error=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"some useful name\",\n",
    "    description=\"some useful description\",\n",
    ")\n",
    "await tool.acall(\n",
    "    \"Responds about research on the impact of AI on the future of work and society?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOS_NmvReUTi"
   },
   "source": [
    "## Creating Toolspecs\n",
    "\n",
    "Let's create a `ToolSpec` from the `GmailToolSpec` from the LlamaHub and convert it to a list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gioBPjcGeUTi",
    "outputId": "50654c7d-39a7-4216-87ca-a21f1ae993cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x7e44eca80e50>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7e43964d9a90>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7e43964dacd0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7e43964dbf90>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7e4395419e50>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7e4395419790>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7EYrQ9GeUTj"
   },
   "source": [
    "To get a more detailed view of the tools, we can take a look at the `metadata` of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uBa7_MceUTj",
    "outputId": "3b3fd2e4-8816-4ab5-bacc-60052471a3e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load_data',\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " ('search_messages',\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\nSearches email messages given a query string and the maximum number\\n        of results requested by the user\\n           Returns: List of relevant message objects up to the maximum number of results.\\n\\n        Args:\\n            query[str]: The user's query\\n            max_results (Optional[int]): The maximum number of search results\\n            to return.\\n        \"),\n",
       " ('create_draft',\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\nCreate and insert a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n        \"),\n",
       " ('update_draft',\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\nUpdate a draft email.\\n           Print the returned draft's message and id.\\n           This function is required to be passed a draft_id that is obtained when creating messages\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            to (Optional[str]): The email addresses to send the message to\\n            subject (Optional[str]): The subject for the event\\n            message (Optional[str]): The message for the event\\n            draft_id (str): the id of the draft to be updated\\n        \"),\n",
       " ('get_draft',\n",
       "  \"get_draft(draft_id: str = None) -> str\\nGet a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \"),\n",
       " ('send_draft',\n",
       "  \"send_draft(draft_id: str = None) -> str\\nSends a draft email.\\n           Print the returned draft's message and id.\\n           Returns: Draft object, including draft id and message meta data.\\n\\n        Args:\\n            draft_id (str): the id of the draft to be updated\\n        \")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
