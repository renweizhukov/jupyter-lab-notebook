{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e879c026-9403-4484-ab88-12b4ac821f99",
   "metadata": {},
   "source": [
    "* https://huggingface.co/learn/nlp-course/en/chapter5/6?fw=pt\n",
    "* https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f36444-2b98-4a72-95e3-ee63db52c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: evaluate in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (4.48.0)\n",
      "Requirement already satisfied: filelock in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers[sentencepiece]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from transformers[sentencepiece]) (5.29.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "664e0d13-c294-4891-bca7-35982b218a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-gpu-cu12 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from faiss-gpu-cu12) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from faiss-gpu-cu12) (24.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from faiss-gpu-cu12) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from faiss-gpu-cu12) (12.6.4.1)\n"
     ]
    }
   ],
   "source": [
    "# faiss-gpu does not work with CUDA 12, so we install faiss-gpu-cu12: https://pypi.org/project/faiss-gpu-cu12/\n",
    "!pip install faiss-gpu-cu12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efca064-720d-499d-84c9-22836fc941be",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61acb52e-f5a5-4c18-a14c-92e9b2bad901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'sub_issues_summary', 'active_lock_reason', 'draft', 'pull_request', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"renwei2024/nemo-github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f280755-ab20-405a-9862-839d0dd49991",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c5a424-0955-4fc3-9c17-6b6173b62b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd50988dcae4cf694f82943b4a0ca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'sub_issues_summary', 'active_lock_reason', 'draft', 'pull_request', 'body', 'closed_by', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'is_pull_request'],\n",
       "    num_rows: 783\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and x[\"comments\"])\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7300435-9bf8-4d65-acbe-c768914eaf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 783\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237f154b-5418-40e9-ba73-af5719d9fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560b188d-6381-4637-8225-4fcc9cc35610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And if I change `punctuation_en_distilbert.nemo` to `punctuation_en_bert.nemo` than .onnx model has 3 inputs, but the issue with \\r\\n`onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(float)) , expected: (tensor(int64))` for `\"attention_mask\"` still exist',\n",
       " 'In forward DistilBERT takes only input_ids and attention_mask (no token_type_ids as in BERT).\\r\\n@borisfom could you help with the ONNX part?',\n",
       " 'Well, finally I ended up with the next code:\\r\\n\\r\\nExport .onnx and config yaml:\\r\\n\\r\\n```\\r\\nmodel = PunctuationCapitalizationModel.restore_from(restore_path=\"punctuation_en_distilbert.nemo\")\\r\\nmodel.export(\"punctuation_en_distilbert.onnx\")\\r\\n\\r\\nmodel = PunctuationCapitalizationModel.restore_from(restore_path=\"punctuation_en_distilbert.nemo\", return_config = True)\\r\\ntextfile = open(\"punctuation_en_distilbert.yaml\", \"w\")\\r\\ntextfile.write(str(OmegaConf.to_yaml(model)))\\r\\ntextfile.close()\\r\\n```\\r\\n\\r\\nExport tokenizer:\\r\\n```\\r\\nfrom transformers import BertTokenizer\\r\\ntokenizer = BertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\r\\ntokenizer.save_pretrained(\"./my_model_directory/\")\\r\\n```\\r\\nand rename `tokenizer_config.json` to `distilbert-base-uncased.json`\\r\\nAlso need to download https://huggingface.co/distilbert-base-uncased/blob/main/tokenizer.json to the my_model_directory folder and change `distilbert-base-uncased.json` content a bit, set `\"tokenizer_file\"` to `\"tokenizer.json\"`\\r\\nNow make processing with onnx:\\r\\n\\r\\n```\\r\\nimport numpy as np\\r\\nimport torch\\r\\nimport yaml\\r\\nfrom nemo.collections.common.tokenizers import AutoTokenizer\\r\\nfrom nemo.collections.nlp.data.token_classification.punctuation_capitalization_dataset import \\\\\\r\\n    BertPunctuationCapitalizationInferDataset\\r\\nimport onnxruntime\\r\\n\\r\\nimport logging\\r\\nlogging.getLogger(\\'nemo_logger\\').setLevel(logging.ERROR)\\r\\n\\r\\ndef create_infer_dataloader(tokenizer, queries):\\r\\n    batch_size = len(queries)\\r\\n\\r\\n    dataset = BertPunctuationCapitalizationInferDataset(\\r\\n        tokenizer=tokenizer, queries=queries, max_seq_length=512\\r\\n    )\\r\\n    return torch.utils.data.DataLoader(\\r\\n        dataset=dataset,\\r\\n        collate_fn=dataset.collate_fn,\\r\\n        batch_size=batch_size,\\r\\n        shuffle=False,\\r\\n        num_workers=1,\\r\\n        drop_last=False,\\r\\n    )\\r\\n\\r\\ndef to_numpy(tensor):\\r\\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\\r\\n\\r\\nqueries = [\"by the end of no such thing the audience like beatrice has a watchful affection for the monster\", \\\\\\r\\n           \"let me guess you\\'re the kind of guy that ignores the rules cause it makes you feel in control am i right \\\\\\r\\n           you\\'re not wrong you think that\\'s cute do you think it\\'s cute\"]\\r\\n\\r\\ntokenizer = AutoTokenizer(pretrained_model_name=\"./my_model_directory/distilbert-base-uncased.json\", \\\\\\r\\n                          vocab_file=\"./my_model_directory/vocab.txt\")\\r\\n\\r\\ndata_loader = create_infer_dataloader(tokenizer, queries)\\r\\n\\r\\nwith open(\"punctuation_en_distilbert.yaml\") as f:\\r\\n    params = yaml.safe_load(f)\\r\\n\\r\\nall_punct_preds = []\\r\\nall_capit_preds = []\\r\\n\\r\\noptions = onnxruntime.SessionOptions()\\r\\noptions.intra_op_num_threads = 1\\r\\n\\r\\nsess_punct = onnxruntime.InferenceSession(\"punctuation_en_distilbert.onnx\", options, providers=[\"CPUExecutionProvider\"])  \\r\\n\\r\\n# most of processing was taken from \\r\\n# https://github.com/NVIDIA/NeMo/blob/5839aee402f314aa413b28e9042b1e1cac10a114/nemo/collections/nlp/models/token_classification/punctuation_capitalization_model.py#L403\\r\\nfor batch in data_loader:\\r\\n    input_ids, input_type_ids, input_mask, subtokens_mask = batch\\r\\n    input_mask = input_mask.type(torch.int64)  # why?\\r\\n    ort_inputs = {\"attention_mask\": to_numpy(input_mask), \"input_ids\": to_numpy(input_ids)}\\r\\n\\r\\n    punct_logits, capit_logits = sess_punct.run(None, input_feed = ort_inputs)\\r\\n\\r\\n    subtokens_mask = subtokens_mask > 0.5\\r\\n\\r\\n    punct_preds = [\\r\\n        list(p_l[subtokens_mask[i]]) for i, p_l in enumerate(np.argmax(punct_logits, axis=-1))\\r\\n    ]\\r\\n    capit_preds = [\\r\\n        list(c_l[subtokens_mask[i]]) for i, c_l in enumerate(np.argmax(capit_logits, axis=-1))\\r\\n    ]\\r\\n\\r\\n    all_punct_preds.extend(punct_preds)\\r\\n    all_capit_preds.extend(capit_preds)\\r\\n\\r\\npunct_ids_to_labels = {v: k for k, v in params[\\'punct_label_ids\\'].items()}\\r\\ncapit_ids_to_labels = {v: k for k, v in params[\\'capit_label_ids\\'].items()}\\r\\n\\r\\nqueries = [q.strip().split() for q in queries]\\r\\n\\r\\nresult = []\\r\\n\\r\\nfor i, query in enumerate(queries):\\r\\n    punct_preds = all_punct_preds[i]\\r\\n    capit_preds = all_capit_preds[i]\\r\\n    if len(query) != len(punct_preds):\\r\\n        # removing the end of phrase punctuation of the truncated segment\\r\\n        punct_preds[-1] = 0\\r\\n        max_len = len(punct_preds)\\r\\n        query = query[:max_len]\\r\\n\\r\\n    query_with_punct_and_capit = \\'\\'\\r\\n    for j, word in enumerate(query):\\r\\n        punct_label = punct_ids_to_labels[punct_preds[j]]\\r\\n        capit_label = capit_ids_to_labels[capit_preds[j]]\\r\\n\\r\\n        if capit_label != params[\\'dataset\\'][\\'pad_label\\']:\\r\\n            word = word.capitalize()\\r\\n        query_with_punct_and_capit += word\\r\\n        if punct_label != params[\\'dataset\\'][\\'pad_label\\']:\\r\\n            query_with_punct_and_capit += punct_label\\r\\n        query_with_punct_and_capit += \\' \\'\\r\\n\\r\\n    result.append(query_with_punct_and_capit.strip())\\r\\n```\\r\\n\\r\\nnot sure if I did everything correct with tokenizer export and import, but it works ',\n",
       " 'I am still facing the same issue after following the steps. ValueError: Model requires 3 inputs. Input Feed contains 2 \\r\\nDid I miss something?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4570994-8327-498b-a546-49861d879995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/NVIDIA/NeMo/issues/2327</td>\n",
       "      <td>Exporting PunctuationCapitalizationModel model...</td>\n",
       "      <td>And if I change `punctuation_en_distilbert.nem...</td>\n",
       "      <td>I am trying to export the nemo_nlp Punctuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/NVIDIA/NeMo/issues/2327</td>\n",
       "      <td>Exporting PunctuationCapitalizationModel model...</td>\n",
       "      <td>In forward DistilBERT takes only input_ids and...</td>\n",
       "      <td>I am trying to export the nemo_nlp Punctuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/NVIDIA/NeMo/issues/2327</td>\n",
       "      <td>Exporting PunctuationCapitalizationModel model...</td>\n",
       "      <td>Well, finally I ended up with the next code:\\r...</td>\n",
       "      <td>I am trying to export the nemo_nlp Punctuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/NVIDIA/NeMo/issues/2327</td>\n",
       "      <td>Exporting PunctuationCapitalizationModel model...</td>\n",
       "      <td>I am still facing the same issue after followi...</td>\n",
       "      <td>I am trying to export the nemo_nlp Punctuation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     html_url  \\\n",
       "0  https://github.com/NVIDIA/NeMo/issues/2327   \n",
       "1  https://github.com/NVIDIA/NeMo/issues/2327   \n",
       "2  https://github.com/NVIDIA/NeMo/issues/2327   \n",
       "3  https://github.com/NVIDIA/NeMo/issues/2327   \n",
       "\n",
       "                                               title  \\\n",
       "0  Exporting PunctuationCapitalizationModel model...   \n",
       "1  Exporting PunctuationCapitalizationModel model...   \n",
       "2  Exporting PunctuationCapitalizationModel model...   \n",
       "3  Exporting PunctuationCapitalizationModel model...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  And if I change `punctuation_en_distilbert.nem...   \n",
       "1  In forward DistilBERT takes only input_ids and...   \n",
       "2  Well, finally I ended up with the next code:\\r...   \n",
       "3  I am still facing the same issue after followi...   \n",
       "\n",
       "                                                body  \n",
       "0  I am trying to export the nemo_nlp Punctuation...  \n",
       "1  I am trying to export the nemo_nlp Punctuation...  \n",
       "2  I am trying to export the nemo_nlp Punctuation...  \n",
       "3  I am trying to export the nemo_nlp Punctuation...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cfcab9d-b10e-49c8-a5df-60e3999d2d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2825\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfaf406-9e23-4ef8-a94f-484ca5ef5fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c920621cbba14524a94cf8bc1b33418b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5c3e78d-cb40-4a7e-8666-be3ca2936718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e6867dbe464703ac39b1848911ce3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2234\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6209b5-aaa6-4330-9df1-ced72820cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(examples):\n",
    "    title = examples[\"title\"] if examples[\"title\"] else \"\"\n",
    "    body = examples[\"body\"] if examples[\"body\"] else \"\"\n",
    "    comments = examples[\"comments\"] if examples[\"comments\"] else \"\"\n",
    "    return {\"text\": title + \" \\n \" + body + \" \\n \" + comments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e5882a-6955-4945-93ec-5addf0e59590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef785eeb2df043aeb816663748188bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abb911-1b17-4ab9-ae5b-8fbf39ff4905",
   "metadata": {},
   "source": [
    "# Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce444823-7402-4814-8664-55a8beda27d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5745042dc2ed415396dbda11cdf9a0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81334274cd7f4a088d5eecff442110f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e01d9c14da74bd7807ffaecd80e0124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2abdbacec043d9a2e9fcce5bf1c33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae958606319747e1bf837fd6a738e603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b083007a4c9846599b6d8295923b2fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6de32cc-2e4c-4a1b-ac83-81d99dc635da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d69ea44-eed0-4cf3-874d-742f541d7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c38b9cd7-2316-4abc-bd7d-dfff34e41ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e89a17ff-ce13-4453-95e5-536eca25904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ab34cec-1859-4558-bb4b-54271ae93519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516293899f424e5b9709ef74850db088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b8dc152-0951-4c30-bf0f-15de957a18bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2234\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be090a-1a9b-43a6-a96f-9cb26c0bbb33",
   "metadata": {},
   "source": [
    "# Use FAISS for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "672cce2d-ae15-4c50-8915-71185cedb11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d2afef5c9e4804b940060da77509c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2234\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "870dd404-223b-4572-97f5-384dc7742e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I build a NeMo docker image?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "089b10ba-5e03-4696-a676-761e61644ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b424198b-3f45-4f16-b650-fd6d1101bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf84453e-037e-49d8-b478-14fb9dc769b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: @wheynelau thanks for your reply.\n",
      "That is what I'm doing. Here is my dockerfile:\n",
      "\n",
      "```dorckerfile\n",
      "FROM nvcr.io/nvidia/nemo:24.12 AS base\n",
      "EXPOSE 8080\n",
      "\n",
      "# Install PowerShell Core\n",
      "RUN apt-get update && apt-get install -y wget \\\n",
      "    && wget -q https://packages.microsoft.com/config/ubuntu/24.04/packages-microsoft-prod.deb \\\n",
      "    && dpkg -i packages-microsoft-prod.deb \\\n",
      "    && apt-get update \\\n",
      "    && apt-get install -y powershell \\\n",
      "    && rm packages-microsoft-prod.deb \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install PowerShell Core\n",
      "RUN apt-get update && apt-get install -y wget \\\n",
      "    && wget -q https://packages.microsoft.com/config/ubuntu/24.04/packages-microsoft-prod.deb \\\n",
      "    && dpkg -i packages-microsoft-prod.deb \\\n",
      "    && apt-get update \\\n",
      "    && apt-get install -y powershell \\\n",
      "    && rm packages-microsoft-prod.deb \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install dependencies aspnetcore\n",
      "RUN apt-get update && apt-get install -y \\\n",
      "    apt-transport-https \\\n",
      "    software-properties-common\n",
      "\n",
      "# Download and run the Microsoft installation script\n",
      "RUN wget https://dot.net/v1/dotnet-install.sh -O dotnet-install.sh \\\n",
      "    && chmod +x dotnet-install.sh \\\n",
      "    && ./dotnet-install.sh --runtime aspnetcore --version 9.0.0\n",
      "\n",
      "# Set the environment variables \n",
      "ENV DOTNET_ROOT=/root/.dotnet \n",
      "ENV PATH=$PATH:/root/.dotnet\n",
      "\n",
      "WORKDIR /app\n",
      "COPY ../Diarization/requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "COPY ../Diarization/audio ./audio\n",
      "\n",
      "FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build\n",
      "ARG BUILD_CONFIGURATION=Release\n",
      "WORKDIR /src\n",
      "COPY [\"Worker/Worker.csproj\", \"Worker/\"]\n",
      "RUN dotnet restore \"./Worker/Worker.csproj\"\n",
      "COPY . .\n",
      "WORKDIR \"/src/Worker\"\n",
      "RUN dotnet build \"./Worker.csproj\" -c $BUILD_CONFIGURATION -o /app/build\n",
      "\n",
      "FROM build AS publish\n",
      "ARG BUILD_CONFIGURATION=Release\n",
      "RUN dotnet publish \"./Worker.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false\n",
      "\n",
      "FROM base AS final\n",
      "WORKDIR /app\n",
      "COPY --from=publish /app/publish .\n",
      "ENTRYPOINT [\"dotnet\", \"Worker.dll\"]\n",
      "\n",
      "```\n",
      "\n",
      "And here is my requirements.txt file:\n",
      "\n",
      "```\n",
      "faster-whisper\n",
      "pydub\n",
      "demucs\n",
      "soundfile\n",
      "Cython\n",
      "packaging\n",
      "```\n",
      "\n",
      "This image has 97.35GB and this is the issue. :)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SCORE: 25.768566131591797\n",
      "TITLE: How to use nemo docker container as base image\n",
      "URL: https://github.com/NVIDIA/NeMo/issues/11824\n",
      "==================================================\n",
      "\n",
      "COMMENT: @wheynelau thanks for your reply.\n",
      "That is what I'm doing. Here is my dockerfile:\n",
      "\n",
      "```dorckerfile\n",
      "FROM nvcr.io/nvidia/nemo:24.12 AS base\n",
      "EXPOSE 8080\n",
      "\n",
      "# Install PowerShell Core\n",
      "RUN apt-get update && apt-get install -y wget \\\n",
      "    && wget -q https://packages.microsoft.com/config/ubuntu/24.04/packages-microsoft-prod.deb \\\n",
      "    && dpkg -i packages-microsoft-prod.deb \\\n",
      "    && apt-get update \\\n",
      "    && apt-get install -y powershell \\\n",
      "    && rm packages-microsoft-prod.deb \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install PowerShell Core\n",
      "RUN apt-get update && apt-get install -y wget \\\n",
      "    && wget -q https://packages.microsoft.com/config/ubuntu/24.04/packages-microsoft-prod.deb \\\n",
      "    && dpkg -i packages-microsoft-prod.deb \\\n",
      "    && apt-get update \\\n",
      "    && apt-get install -y powershell \\\n",
      "    && rm packages-microsoft-prod.deb \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Install dependencies aspnetcore\n",
      "RUN apt-get update && apt-get install -y \\\n",
      "    apt-transport-https \\\n",
      "    software-properties-common\n",
      "\n",
      "# Download and run the Microsoft installation script\n",
      "RUN wget https://dot.net/v1/dotnet-install.sh -O dotnet-install.sh \\\n",
      "    && chmod +x dotnet-install.sh \\\n",
      "    && ./dotnet-install.sh --runtime aspnetcore --version 9.0.0\n",
      "\n",
      "# Set the environment variables \n",
      "ENV DOTNET_ROOT=/root/.dotnet \n",
      "ENV PATH=$PATH:/root/.dotnet\n",
      "\n",
      "WORKDIR /app\n",
      "COPY ../Diarization/requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "COPY ../Diarization/audio ./audio\n",
      "\n",
      "FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build\n",
      "ARG BUILD_CONFIGURATION=Release\n",
      "WORKDIR /src\n",
      "COPY [\"Worker/Worker.csproj\", \"Worker/\"]\n",
      "RUN dotnet restore \"./Worker/Worker.csproj\"\n",
      "COPY . .\n",
      "WORKDIR \"/src/Worker\"\n",
      "RUN dotnet build \"./Worker.csproj\" -c $BUILD_CONFIGURATION -o /app/build\n",
      "\n",
      "FROM build AS publish\n",
      "ARG BUILD_CONFIGURATION=Release\n",
      "RUN dotnet publish \"./Worker.csproj\" -c $BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false\n",
      "\n",
      "FROM base AS final\n",
      "WORKDIR /app\n",
      "COPY --from=publish /app/publish .\n",
      "ENTRYPOINT [\"dotnet\", \"Worker.dll\"]\n",
      "\n",
      "```\n",
      "\n",
      "And here is my requirements.txt file:\n",
      "\n",
      "```\n",
      "faster-whisper\n",
      "pydub\n",
      "demucs\n",
      "soundfile\n",
      "Cython\n",
      "packaging\n",
      "```\n",
      "\n",
      "This image has 97.35GB and this is the issue. :)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SCORE: 25.768566131591797\n",
      "TITLE: How to use nemo docker container as base image\n",
      "URL: https://github.com/NVIDIA/NeMo/issues/11824\n",
      "==================================================\n",
      "\n",
      "COMMENT: Hi, you can use this as the start of your dockerfile\n",
      "\n",
      "```\n",
      "ARG NEMOFW_VERSION=24.09\n",
      "FROM nvcr.io/nvidia/nemo:$NEMOFW_VERSION\n",
      "```\n",
      "SCORE: 25.037113189697266\n",
      "TITLE: How to use nemo docker container as base image\n",
      "URL: https://github.com/NVIDIA/NeMo/issues/11824\n",
      "==================================================\n",
      "\n",
      "COMMENT: Hi, you can use this as the start of your dockerfile\n",
      "\n",
      "```\n",
      "ARG NEMOFW_VERSION=24.09\n",
      "FROM nvcr.io/nvidia/nemo:$NEMOFW_VERSION\n",
      "```\n",
      "SCORE: 25.037113189697266\n",
      "TITLE: How to use nemo docker container as base image\n",
      "URL: https://github.com/NVIDIA/NeMo/issues/11824\n",
      "==================================================\n",
      "\n",
      "COMMENT: Are you using ` docker build - < Dockerfile` ?  @asher-bs \n",
      "\n",
      "> \"This will read a Dockerfile from STDIN without context. Due to the lack of a context, no contents of any local directory will be sent to the Docker daemon. Since there is no context, a Dockerfile ADD only works if it refers to a remote URL.\"\n",
      "\n",
      "Please use `docker build -f Dockerfile -t <container_name> .` as suggested in  https://github.com/NVIDIA/NeMo/pull/3223 \n",
      "SCORE: 23.69618797302246\n",
      "TITLE: Docker can't be build using last Dockerfile\n",
      "URL: https://github.com/NVIDIA/NeMo/issues/2040\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8377e-d904-47ad-a92f-9a298b254503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
