{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c438fb1a-0804-4274-84cc-c7ccf511af06",
   "metadata": {},
   "source": [
    "We mostly follow the blog [\"Transformers Laid Out\"](https://goyalpramod.github.io/blogs/Transformers_laid_out/) to implement a vanilla transformer presented in the seminal paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) but make the following changes:\n",
    "\n",
    "1. Use separate dropouts after multi-headed attention and feed-forward network in `EncoderLayer` and `DecoderLayer`.\n",
    "2. Query may have different sequence length from Key and Value in `MultiHeadAttention.forward`.\n",
    "4. Use logical or operation to combine the padding mask and future mask.\n",
    "5. Work around the `RuntimeError`: \"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\" in `create_masks` method.\n",
    "6. Fix 2 issues in `TransformerLRScheduler.step()`.\n",
    "7. As for the soft target distribution of Label Smoothing, set the probability of the true label to `(1 - smoothing) + smoothing / vocab_size` and the probability of other tokens to `smoothing / vocab_size`.\n",
    "8. Remove the redundant `create_mask` in method `training_transformer` since we create the source and target masks in `Transformer.forward`.\n",
    "9. Add the missing `AdamW` optimizer for training the transformer.\n",
    "10. Replace `view()` with `reshape()` to reshape `tgt_output` in `train_transformer` since `tgt_output` is not contiguous.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "1. Use the data loader in HuggingFace `datasets` library.\n",
    "2. Use Accelerate to train the transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfcad0f-4415-4d31-a8d5-08d3f0f0145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b376cfe-a2db-4052-920a-5c78d1b9a3f0",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418a0c48-5b68-4d54-b742-74a92ebec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query: (batch_size, num_heads, seq_len_q, d_q)\n",
    "        key: (batch_size, num_heads, seq_len_k, d_k)\n",
    "        value: (batch_size, num_heads, seq_len_v, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "    \"\"\"\n",
    "    # Shape checks\n",
    "    assert query.dim() == 4, f\"Query should be 4-dim but got {query.dim()}-dim\"\n",
    "    assert key.dim() == 4, f\"Key should be 4-dim but got {key.dim()}-dim\"\n",
    "    assert value.dim() == 4, f\"Value should be 4-dim but got {value.dim()}-dim\"\n",
    "    assert query.size(-1) == key.size(\n",
    "        -1\n",
    "    ), f\"Query depth {query.size(-1)} != Key depth {key.size(-1)}\"\n",
    "    assert key.size(-2) == value.size(\n",
    "        -2\n",
    "    ), f\"Key length {key.size(-2)} != Value length {value.size(-2)}\"\n",
    "\n",
    "    # Get the Key depth.\n",
    "    d_k = key.size(-1)\n",
    "\n",
    "    # Calculate the attention scores from Query and Key.\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # Set the masked elements to -inf for the decoder, which are usually the upper right half.\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    # Calculate the attention weights by taking the softmax for each Query token along Keys.\n",
    "    weights = softmax(scores, dim=-1)\n",
    "\n",
    "    # Return the mulplication of the attention weights and Value.\n",
    "    return torch.matmul(weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b2d009b-1077-4cdc-9b8c-4d6260f10c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            d_model % num_heads == 0\n",
    "        ), f\"d_model = {d_model} is not divisible by num_heads = {num_heads}\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Create the learnable projection matrices\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, num_heads, seq_len_q, d_q)\n",
    "            key: (batch_size, num_heads, seq_len_k, d_k)\n",
    "            value: (batch_size, num_heads, seq_len_v, d_v)\n",
    "            mask: Optional mask to prevent attention to certain positions\n",
    "        \"\"\"\n",
    "        # Shape checks\n",
    "        assert query.dim() == 4, f\"Query should be 4-dim but got {query.dim()}-dim\"\n",
    "        assert key.dim() == 4, f\"Key should be 4-dim but got {key.dim()}-dim\"\n",
    "        assert value.dim() == 4, f\"Value should be 4-dim but got {value.dim()}-dim\"\n",
    "        assert query.size(-1) == key.size(\n",
    "            -1\n",
    "        ), f\"Query depth {query.size(-1)} != Key depth {key.size(-1)}\"\n",
    "        assert key.size(-2) == value.size(\n",
    "            -2\n",
    "        ), f\"Key length {key.size(-2)} != Value length {value.size(-2)}\"\n",
    "\n",
    "        # Get the Key depth.\n",
    "        d_k = key.size(-1)\n",
    "\n",
    "        # Calculate the attention scores from Query and Key.\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # Set the masked elements to -inf for the decoder, which are usually the upper right half.\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Calculate the attention weights by taking the softmax for each Query token along Keys.\n",
    "        weights = softmax(scores, dim=-1)\n",
    "\n",
    "        # Return the mulplication of the attention weights and Value.\n",
    "        return torch.matmul(weights, value)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, query_seq_len, d_model)\n",
    "            key: (batch_size, key_seq_len, d_model)\n",
    "            value: (batch_size, key_seq_len, d_model)\n",
    "            mask: Optional mask to prevent attention to certain positions\n",
    "        \"\"\"\n",
    "        assert key.size(1) == value.size(\n",
    "            1\n",
    "        ), f\"key sequence length {key.size(1)} != value sequence length {value.size(1)}\"\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "        query_seq_len = query.size(1)\n",
    "        key_seq_len = key.size(1)\n",
    "\n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(query)  # (batch_size, query_seq_len, d_model)\n",
    "        K = self.W_k(key)  # (batch_size, key_seq_len, d_model)\n",
    "        V = self.W_v(value)  # (batch_size, key_seq_len, d_model)\n",
    "\n",
    "        # 2. Split into heads with the shape (batch_size, num_heads, seq_len, d_k).\n",
    "        # Note that d_model = num_heads * d_k.\n",
    "        Q = Q.view(batch_size, query_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, key_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, key_seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Apply scaled dot product attention with the shape (batch_size, num_heads, seq_len, d_k).\n",
    "        self_attention = MultiHeadAttention.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 4. Concatenate num_heads (batch_size, query_seq_len, d_k) into (batch_size, query_seq_len, d_model).\n",
    "        # Note that d_model = num_heads * d_k.\n",
    "        concatenated_self_attention = (\n",
    "            self_attention.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, query_seq_len, self.d_model)\n",
    "        )\n",
    "\n",
    "        # 5. Final projection\n",
    "        return self.W_o(concatenated_self_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c28ea-b78c-43f9-8f08-be243ee4d2be",
   "metadata": {},
   "source": [
    "# Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63673505-db04-4ea0-81f3-6cf6f762943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network\n",
    "    Args:\n",
    "        d_model: input/output dimension\n",
    "        d_ff: hidden dimension\n",
    "        dropout: dropout rate (default=0.1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # Output shape: (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),  # Output shape: (batch_size, seq_len, d_model)\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efa0c5-77f4-45dd-9082-8a1a809ba662",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c93a11-43b3-431e-9b07-ac9b4f3b6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a matrix of shape (max_seq_length, d_model).\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # Create a position column vector of shape (max_seq_length, 1).\n",
    "        pos = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "\n",
    "        # Create a division 1-D array of shape (d_model // 2).\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Compute the matrix of positional encodings.\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "        # Register buffer with shape (1, max_seq_length, d_model).\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, : x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc77569-13f0-4c29-a48a-3279693d86b5",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffe412b8-4129-4f7e-8f02-35b53a1cf6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Multi-head attention\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # 2. Dropout\n",
    "        self.dropout_mha = nn.Dropout(dropout)\n",
    "\n",
    "        # 3. Layer normalization\n",
    "        self.layer_normal_mha = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 4. Feed forward\n",
    "        self.ff = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "\n",
    "        # 5. Another Dropout\n",
    "        self.dropout_ff = nn.Dropout(dropout)\n",
    "\n",
    "        # 6. Another layer normalization\n",
    "        self.layer_normal_ff = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask for padding\n",
    "        Returns:\n",
    "            x: Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Multi-head attention with residual connection and layer norm\n",
    "        mha_output = self.mha(x, x, x, mask)\n",
    "        x = self.dropout_mha(x + mha_output)\n",
    "        x = self.layer_normal_mha(x)\n",
    "\n",
    "        # 2. Feed forward with residual connection and layer norm\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.dropout_ff(x + ff_output)\n",
    "        x = self.layer_normal_ff(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b24f52-535f-4de7-9e36-f5b3b25bd867",
   "metadata": {},
   "source": [
    "# Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dcaea07-31ab-4b63-adff-9d1fe07858fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Masked Multi-Head Attention\n",
    "        self.masked_mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # 2. Dropout for Masked Multi-Head Attention\n",
    "        self.dropout_masked_mha = nn.Dropout(dropout)\n",
    "\n",
    "        # 3. Layer norm for first sub-layer\n",
    "        self.layer_normal_masked_mha = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 4. Multi-Head Attention for cross attention with encoder output\n",
    "        # This will take encoder output as key and value\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # 5. Dropout for Multi-Head Attention\n",
    "        self.dropout_mha = nn.Dropout(dropout)\n",
    "\n",
    "        # 6. Layer norm for second sub-layer\n",
    "        self.layer_normal_mha = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 7. Feed forward network\n",
    "        self.ff = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "        # 8. Dropout for Feed Forward network\n",
    "        self.dropout_ff = nn.Dropout(dropout)\n",
    "\n",
    "        # 9. Layer norm for third sub-layer\n",
    "        self.layer_normal_ff = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Target sequence embedding (batch_size, target_seq_len, d_model)\n",
    "            encoder_output: Output from encoder (batch_size, source_seq_len, d_model)\n",
    "            src_mask: Mask for source padding\n",
    "            tgt_mask: Mask for target padding and future positions\n",
    "        \"\"\"\n",
    "        # 1. Masked self-attention\n",
    "        masked_mha_output = self.masked_mha(x, x, x, tgt_mask)\n",
    "        x = self.dropout_masked_mha(x + masked_mha_output)\n",
    "        x = self.layer_normal_masked_mha(x)\n",
    "\n",
    "        # 2. Cross-Attention between Query and encoder output as Key and Value.\n",
    "        mha_output = self.mha(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.dropout_mha(x + mha_output)\n",
    "        x = self.layer_normal_mha(x)\n",
    "\n",
    "        # 3. Feed forward network\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.dropout_ff(x + ff_output)\n",
    "        x = self.layer_normal_ff(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77c812-c111-4a92-9f99-2e6aa5a787b7",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5783516f-0428-45b9-a047-b4f105b458da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        max_seq_length=5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Input embedding\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "        # 2. Positional encoding\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # 3. Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 4. Stack of N encoder layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tokens (batch_size, seq_len)\n",
    "            mask: Mask for padding positions\n",
    "        Returns:\n",
    "            encoder_output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Pass through embedding layer and scale\n",
    "        x = self.embeddings(x) * self.scale\n",
    "\n",
    "        # 2. Add positional encoding and apply dropout\n",
    "        x = self.dropout(self.pe(x))\n",
    "\n",
    "        # 3. Pass through each encoder layer\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912231b3-3efa-471b-a61e-3460e3b90dcd",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "244301cb-3a93-40b4-a834-5f047f6aa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        max_seq_length=5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Output embedding\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "        # 2. Positional encoding\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # 3. Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 4. Stack of N decoder layers\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Target tokens (batch_size, target_seq_len)\n",
    "            encoder_output: Output from encoder (batch_size, source_seq_len, d_model)\n",
    "            src_mask: Mask for source padding\n",
    "            tgt_mask: Mask for target padding and future positions\n",
    "        Returns:\n",
    "            decoder_output: (batch_size, target_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Pass through embedding layer and scale\n",
    "        x = self.embeddings(x) * self.scale\n",
    "\n",
    "        # 2. Add positional encoding and dropout\n",
    "        x = self.dropout(self.pe(x))\n",
    "\n",
    "        # 3. Pass through each decoder layer\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af6df4-80e4-41ed-ae53-657fca89c051",
   "metadata": {},
   "source": [
    "# Utility Code for creating masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af520e12-781f-499a-802a-46bc3a3fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Create mask for padding tokens (0s)\n",
    "    Args:\n",
    "        seq: Input sequence tensor (batch_size, seq_len)\n",
    "    Returns:\n",
    "        mask: Padding mask (batch_size, 1, 1, seq_len)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = seq.shape\n",
    "    # Input seq: 0 - padding tokens.\n",
    "    # Output mask: 0 - allowed positions; 1 - padding positions.\n",
    "    output = torch.eq(seq, 0).float()\n",
    "    return output.view(batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bde77d2-c7c8-4301-abb3-f5d63bfaa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_future_mask(size):\n",
    "    \"\"\"\n",
    "    Create mask to prevent attention to future positions\n",
    "    Args:\n",
    "        size: Size of square mask (target_seq_len)\n",
    "    Returns:\n",
    "        mask: Future mask (1, 1, size, size)\n",
    "    \"\"\"\n",
    "    # Create upper triangular matrix and invert it.\n",
    "    # Output mask: 0 - allowed positions; 1 - masked positions.\n",
    "    return torch.triu(torch.ones(1, 1, size, size), diagonal=1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d167c7-3430-4e95-b85e-817dd3f36322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src, tgt):\n",
    "    \"\"\"\n",
    "    Create all masks needed for training\n",
    "    Args:\n",
    "        src: Source sequence (batch_size, src_len)\n",
    "        tgt: Target sequence (batch_size, tgt_len)\n",
    "    Returns:\n",
    "        src_mask: Padding mask for encoder\n",
    "        tgt_mask: Combined padding and future mask for decoder\n",
    "    \"\"\"\n",
    "    # 1. Create padding masks\n",
    "    src_padding_mask = create_padding_mask(src)  # Shape: (batch_size, 1, 1, src_len)\n",
    "    tgt_padding_mask = create_padding_mask(tgt)  # Shape: ()\n",
    "\n",
    "    # 2. Create future mask\n",
    "    tgt_future_mask = create_future_mask(tgt.size(1))\n",
    "\n",
    "    # 3. Combine padding and future mask for target\n",
    "    # Both masks should be 0 for allowed positions\n",
    "    # BUGBUG: Manually copy the future mask to GPU.\n",
    "    tgt_mask = torch.logical_or(tgt_padding_mask, tgt_future_mask.to(\"cuda\"))\n",
    "\n",
    "    return src_padding_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8c541-fa94-4baf-81f7-a6b5e0c5a49f",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cef248d6-6337-4062-bb45-b17bd4d8ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        d_model,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        dropout=0.1,\n",
    "        max_seq_length=5000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Pass all necessary parameters to Encoder and Decoder\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            d_model,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            d_ff,\n",
    "            dropout,\n",
    "            max_seq_length,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            tgt_vocab_size,\n",
    "            d_model,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            d_ff,\n",
    "            dropout,\n",
    "            max_seq_length,\n",
    "        )\n",
    "\n",
    "        # The final linear layer should project from d_model to tgt_vocab_size\n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Create masks for source and target\n",
    "        src_mask, tgt_mask = create_masks(src, tgt)\n",
    "\n",
    "        # Pass through encoder\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "\n",
    "        # Pass through decoder\n",
    "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        output = self.final_layer(decoder_output)\n",
    "\n",
    "        # Note: Usually don't apply softmax here if using CrossEntropyLoss\n",
    "        # as it applies log_softmax internally\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75219cb7-57a3-4ca9-9e6b-406a5dd16e18",
   "metadata": {},
   "source": [
    "# Utility code for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4faef284-55f7-406e-8e16-4cfe7239b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLRScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer: Optimizer to adjust learning rate for\n",
    "            d_model: Model dimensionality\n",
    "            warmup_steps: Number of warmup steps\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0  # Track the number of steps\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update learning rate based on step number\n",
    "        \"\"\"\n",
    "        # Increment step count\n",
    "        self.step_num += 1\n",
    "\n",
    "        # Convert integers to tensors\n",
    "        step_num_tensor = torch.tensor(self.step_num, dtype=torch.float32)\n",
    "        warmup_steps_tensor = torch.tensor(self.warmup_steps, dtype=torch.float32)\n",
    "        d_model_tensor = torch.tensor(self.d_model, dtype=torch.float32)\n",
    "\n",
    "        # Copy the learning rate formula from Section 5.3 of paper \"Attention is All You Need\":\n",
    "        #   lrate = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))\n",
    "        lrate = torch.pow(d_model_tensor, -0.5) * torch.min(\n",
    "            torch.pow(step_num_tensor, -0.5),\n",
    "            step_num_tensor * torch.pow(warmup_steps_tensor, -1.5),\n",
    "        )\n",
    "\n",
    "        # Apply new learning rate to optimizer\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lrate.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98ac94c-fc12-43a3-82ba-3b4d8f14232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    # One regularization technique mentioned in Section 5.4 of paper \"Attention is All You Need\".\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Model predictions (batch_size * seq_len, vocab_size) #each row of vocab_size contains probability score of each label\n",
    "            target: True labels (batch_size * seq_len) #each row of batch size contains the index to the correct label\n",
    "        \"\"\"\n",
    "        vocab_size = logits.size(-1)\n",
    "        with torch.no_grad():\n",
    "            # Create a soft target distribution\n",
    "            true_dist = torch.zeros_like(\n",
    "                logits\n",
    "            )  # create the all-zero tensor with the same shape as logits\n",
    "            true_dist.fill_(\n",
    "                self.smoothing / vocab_size\n",
    "            )  # set the probability of tokens other than the true label\n",
    "            true_dist.scatter_(\n",
    "                1, target.unsqueeze(1), self.confidence + self.smoothing / vocab_size\n",
    "            )  # set the probability of the true label\n",
    "\n",
    "        # Return cross entropy loss. Note that we use log_softmax instead of log + softmax for stability (i.e., avoid overflow\n",
    "        # and underflow) and efficiency.\n",
    "        return torch.mean(\n",
    "            torch.sum(-true_dist * torch.log_softmax(logits, dim=-1), dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f99c2-415b-4a36-b7e2-a24d952394ca",
   "metadata": {},
   "source": [
    "# Training the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa28a9eb-97fe-4d3f-808e-5deccb6104ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(\n",
    "    model, train_dataloader, criterion, optimizer, scheduler, num_epochs, device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for transformer\n",
    "\n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        train_dataloader: DataLoader for training data\n",
    "        criterion: Loss function (with label smoothing)\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        num_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    # 1. Setup\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # For tracking training progress:\n",
    "    total_loss = 0\n",
    "    all_losses = []\n",
    "\n",
    "    # 2. Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Get source and target batches\n",
    "            src = batch[\"src\"].to(device)\n",
    "            tgt = batch[\"tgt\"].to(device)\n",
    "\n",
    "            # Prepare target for input and output\n",
    "            # Remove last token from target for input\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            # Remove first token from target for output\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass. Note that during training, we predict all the tokens in parallel, i.e., in one forward pass.\n",
    "            outputs = model(src, tgt_input)\n",
    "\n",
    "            # Reshape outputs and target for loss calculation.\n",
    "            # outputs shape: (batch_size, seq_len - 1, vocab_size) --> (batch_size * (seq_len - 1), vocab_size)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            # tgt_outputs shape: (batch_size, seq_len - 1) --> (batch_size * (seq_len - 1))\n",
    "            # Note that view(-1) does not work because `tgt_output` is not contiguous.\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, tgt_output)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update loss tracking\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Print progress every N batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate average loss for epoch\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        all_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": avg_epoch_loss,\n",
    "            },\n",
    "            f\"checkpoint_epoch_{epoch + 1}.pt\",\n",
    "        )\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedacc72-f9ab-443c-bcef-5f79622c6f33",
   "metadata": {},
   "source": [
    "# Prepare the dataset and define the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2710b3-cd2c-4867-b95d-94807a9e2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (2.10.5)\n",
      "Requirement already satisfied: jinja2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/renwei/anaconda3/envs/llm/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "698b463b-8345-4db2-b26e-5b7424c18c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "data_dir = \"../data/vanilla-transformer\"\n",
    "\n",
    "\n",
    "def download_multi30k():\n",
    "    \"\"\"Download Multi30k dataset if not present\"\"\"\n",
    "    # Create data directory\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Download files if they don't exist\n",
    "    base_url = (\n",
    "        \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
    "    )\n",
    "    files = {\n",
    "        \"train.de\": \"train.de.gz\",\n",
    "        \"train.en\": \"train.en.gz\",\n",
    "        \"val.de\": \"val.de.gz\",\n",
    "        \"val.en\": \"val.en.gz\",\n",
    "        \"test.de\": \"test_2016_flickr.de.gz\",\n",
    "        \"test.en\": \"test_2016_flickr.en.gz\",\n",
    "    }\n",
    "\n",
    "    for local_name, remote_name in files.items():\n",
    "        filepath = f\"{data_dir}/{local_name}\"\n",
    "        if not os.path.exists(filepath):\n",
    "            url = base_url + remote_name\n",
    "            urllib.request.urlretrieve(url, filepath + \".gz\")\n",
    "            os.system(f\"gunzip -f {filepath}.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c6e4156-5e77-4fe9-adc3-88ba204423e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"Load data from file\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2561fe60-6397-4749-bbfe-7a897929e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"Create dataset from files\"\"\"\n",
    "    # Download data if needed\n",
    "    download_multi30k()\n",
    "\n",
    "    # Load data\n",
    "    train_de = load_data(f\"{data_dir}/train.de\")\n",
    "    train_en = load_data(f\"{data_dir}/train.en\")\n",
    "    val_de = load_data(f\"{data_dir}/val.de\")\n",
    "    val_en = load_data(f\"{data_dir}/val.en\")\n",
    "\n",
    "    return (train_de, train_en), (val_de, val_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b9a1751-42fa-42c4-9fff-9dd94a1ac751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, src_texts, tgt_texts, src_vocab, tgt_vocab, src_tokenizer, tgt_tokenizer\n",
    "    ):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        src_tokens = [tok.text for tok in self.src_tokenizer(src_text)]\n",
    "        tgt_tokens = [tok.text for tok in self.tgt_tokenizer(tgt_text)]\n",
    "\n",
    "        # Convert to indices\n",
    "        src_indices = (\n",
    "            [self.src_vocab[\"<s>\"]]\n",
    "            + [self.src_vocab.get(token, 3) for token in src_tokens]  # \"<unk>\": 3\n",
    "            + [self.src_vocab[\"</s>\"]]\n",
    "        )\n",
    "        tgt_indices = (\n",
    "            [self.tgt_vocab[\"<s>\"]]\n",
    "            + [self.tgt_vocab.get(token, 3) for token in tgt_tokens]  # \"<unk>\": 3\n",
    "            + [self.tgt_vocab[\"</s>\"]]\n",
    "        )\n",
    "\n",
    "        return {\"src\": torch.tensor(src_indices), \"tgt\": torch.tensor(tgt_indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eb826a8-2a08-471a-a005-68d8d6fe6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_texts(texts, tokenizer, min_freq=2):\n",
    "    \"\"\"Build vocabulary from texts\"\"\"\n",
    "    counter = {}\n",
    "    for text in texts:\n",
    "        for token in [tok.text for tok in tokenizer(text)]:\n",
    "            counter[token] = counter.get(token, 0) + 1\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocab = {\"<s>\": 0, \"</s>\": 1, \"<blank>\": 2, \"<unk>\": 3}\n",
    "    idx = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3777a03-9323-41b5-b668-a462966b50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size=32):\n",
    "    # Load tokenizers\n",
    "    spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Get data\n",
    "    (train_de, train_en), (val_de, val_en) = create_dataset()\n",
    "\n",
    "    # Build vocabularies\n",
    "    vocab_src = build_vocab_from_texts(train_de, spacy_de)\n",
    "    vocab_tgt = build_vocab_from_texts(train_en, spacy_en)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TranslationDataset(\n",
    "        train_de, train_en, vocab_src, vocab_tgt, spacy_de, spacy_en\n",
    "    )\n",
    "\n",
    "    val_dataset = TranslationDataset(\n",
    "        val_de, val_en, vocab_src, vocab_tgt, spacy_de, spacy_en\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, vocab_src, vocab_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b31e6f5a-edb2-4d85-b669-c89a8b72a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    src_tensors = [item[\"src\"] for item in batch]\n",
    "    tgt_tensors = [item[\"tgt\"] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        src_tensors, batch_first=True, padding_value=2\n",
    "    )\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        tgt_tensors, batch_first=True, padding_value=2\n",
    "    )\n",
    "\n",
    "    return {\"src\": src_padded, \"tgt\": tgt_padded}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678ecce-ccf1-4594-9715-5d7c450b092d",
   "metadata": {},
   "source": [
    "# Start the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "558091c9-f1cc-4d5e-aaa8-b9e2f8027d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "888b48ed-191e-42b8-9787-4d7319f00eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feba0f98-f8ae-40aa-ba87-ee032e07df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "train_dataloader, val_dataloader, vocab_src, vocab_tgt = create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9eb5d188-0fa0-4ee3-be7b-c5ea72bf9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your transformer with the vocabulary sizes\n",
    "device = \"cuda\"\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(vocab_src),\n",
    "    tgt_vocab_size=len(vocab_tgt),\n",
    "    d_model=512,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1,\n",
    ")\n",
    "criterion = LabelSmoothing(smoothing=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d63a45d5-1b9b-4260-9c6a-28dc4c0a16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer.\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), betas=(0.9, 0.98), eps=1e-9, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "713ff61e-4c24-4628-962f-88c20def20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learning rate scheduler\n",
    "d_model = 512\n",
    "warmup_steps = 4000  # Common warmup step value\n",
    "scheduler = TransformerLRScheduler(optimizer, d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7b4d65f-8066-47b7-b0cf-8702ff91ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Batch 0, Loss: 7.3699\n",
      "Batch 100, Loss: 4.7729\n",
      "Batch 200, Loss: 3.9463\n",
      "Batch 300, Loss: 3.2740\n",
      "Batch 400, Loss: 3.6947\n",
      "Batch 500, Loss: 3.9301\n",
      "Batch 600, Loss: 2.9096\n",
      "Batch 700, Loss: 3.3312\n",
      "Batch 800, Loss: 3.5278\n",
      "Batch 900, Loss: 3.2851\n",
      "Epoch 1, Loss: 3.7799\n",
      "Epoch 2/10\n",
      "Batch 0, Loss: 3.4062\n",
      "Batch 100, Loss: 3.3547\n",
      "Batch 200, Loss: 3.5276\n",
      "Batch 300, Loss: 3.4446\n",
      "Batch 400, Loss: 3.1315\n",
      "Batch 500, Loss: 3.2641\n",
      "Batch 600, Loss: 2.9427\n",
      "Batch 700, Loss: 3.0760\n",
      "Batch 800, Loss: 3.5282\n",
      "Batch 900, Loss: 3.5347\n",
      "Epoch 2, Loss: 3.1905\n",
      "Epoch 3/10\n",
      "Batch 0, Loss: 3.0667\n",
      "Batch 100, Loss: 2.8320\n",
      "Batch 200, Loss: 2.6779\n",
      "Batch 300, Loss: 3.2682\n",
      "Batch 400, Loss: 3.2879\n",
      "Batch 500, Loss: 2.7618\n",
      "Batch 600, Loss: 3.3126\n",
      "Batch 700, Loss: 3.3712\n",
      "Batch 800, Loss: 3.1600\n",
      "Batch 900, Loss: 3.0311\n",
      "Epoch 3, Loss: 3.0968\n",
      "Epoch 4/10\n",
      "Batch 0, Loss: 3.4050\n",
      "Batch 100, Loss: 3.1580\n",
      "Batch 200, Loss: 3.0060\n",
      "Batch 300, Loss: 2.9221\n",
      "Batch 400, Loss: 3.3623\n",
      "Batch 500, Loss: 3.5122\n",
      "Batch 600, Loss: 3.2513\n",
      "Batch 700, Loss: 2.8976\n",
      "Batch 800, Loss: 3.4228\n",
      "Batch 900, Loss: 2.8617\n",
      "Epoch 4, Loss: 3.0687\n",
      "Epoch 5/10\n",
      "Batch 0, Loss: 3.3189\n",
      "Batch 100, Loss: 2.6470\n",
      "Batch 200, Loss: 3.0566\n",
      "Batch 300, Loss: 3.2277\n",
      "Batch 400, Loss: 2.9123\n",
      "Batch 500, Loss: 3.1615\n",
      "Batch 600, Loss: 3.0973\n",
      "Batch 700, Loss: 2.4253\n",
      "Batch 800, Loss: 3.0912\n",
      "Batch 900, Loss: 3.0712\n",
      "Epoch 5, Loss: 3.0870\n",
      "Epoch 6/10\n",
      "Batch 0, Loss: 3.1603\n",
      "Batch 100, Loss: 3.2855\n",
      "Batch 200, Loss: 3.2032\n",
      "Batch 300, Loss: 2.9306\n",
      "Batch 400, Loss: 3.2876\n",
      "Batch 500, Loss: 3.2855\n",
      "Batch 600, Loss: 2.7093\n",
      "Batch 700, Loss: 3.4381\n",
      "Batch 800, Loss: 2.8553\n",
      "Batch 900, Loss: 2.9093\n",
      "Epoch 6, Loss: 3.1435\n",
      "Epoch 7/10\n",
      "Batch 0, Loss: 3.1816\n",
      "Batch 100, Loss: 3.4681\n",
      "Batch 200, Loss: 3.2456\n",
      "Batch 300, Loss: 2.8545\n",
      "Batch 400, Loss: 3.2346\n",
      "Batch 500, Loss: 3.5412\n",
      "Batch 600, Loss: 3.4733\n",
      "Batch 700, Loss: 3.2063\n",
      "Batch 800, Loss: 2.7815\n",
      "Batch 900, Loss: 3.4570\n",
      "Epoch 7, Loss: 3.1944\n",
      "Epoch 8/10\n",
      "Batch 0, Loss: 3.0010\n",
      "Batch 100, Loss: 3.5480\n",
      "Batch 200, Loss: 2.5969\n",
      "Batch 300, Loss: 3.8323\n",
      "Batch 400, Loss: 3.4008\n",
      "Batch 500, Loss: 3.4624\n",
      "Batch 600, Loss: 3.7777\n",
      "Batch 700, Loss: 3.4924\n",
      "Batch 800, Loss: 3.2648\n",
      "Batch 900, Loss: 3.2188\n",
      "Epoch 8, Loss: 3.3480\n",
      "Epoch 9/10\n",
      "Batch 0, Loss: 2.9659\n",
      "Batch 100, Loss: 3.6948\n",
      "Batch 200, Loss: 3.5967\n",
      "Batch 300, Loss: 3.4705\n",
      "Batch 400, Loss: 3.3879\n",
      "Batch 500, Loss: 3.2340\n",
      "Batch 600, Loss: 3.1439\n",
      "Batch 700, Loss: 3.7312\n",
      "Batch 800, Loss: 3.3597\n",
      "Batch 900, Loss: 3.5070\n",
      "Epoch 9, Loss: 3.4862\n",
      "Epoch 10/10\n",
      "Batch 0, Loss: 4.0642\n",
      "Batch 100, Loss: 3.5702\n",
      "Batch 200, Loss: 3.3660\n",
      "Batch 300, Loss: 3.1653\n",
      "Batch 400, Loss: 4.0917\n",
      "Batch 500, Loss: 3.4500\n",
      "Batch 600, Loss: 4.1535\n",
      "Batch 700, Loss: 3.8030\n",
      "Batch 800, Loss: 4.2275\n",
      "Batch 900, Loss: 3.5936\n",
      "Epoch 10, Loss: 3.7030\n"
     ]
    }
   ],
   "source": [
    "# Now you can use your training loop\n",
    "losses = train_transformer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=10,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
